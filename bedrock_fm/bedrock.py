import boto3
import json
import time
from PIL.Image import Image

from attrs import define, field, Factory
from botocore.eventstream import EventStream
from .exceptions import BedrockArgsError
from typing import Any, List, Dict, Iterable, Iterator, Literal, overload, Optional
import logging
from enum import Enum

from abc import abstractmethod

logger = logging.getLogger(__name__)

CONTENT_TYPE_APPLICATION_JSON = "application/json"


class Model(Enum):
    AMAZON_TITAN_TG1_LARGE = "amazon.titan-tg1-large"
    AMAZON_TITAN_EMBED_G1_TEXT_02 = "amazon.titan-embed-g1-text-02"
    AMAZON_TITAN_TEXT_LITE_V1 = "amazon.titan-text-lite-v1"
    AMAZON_TITAN_TEXT_EXPRESS_V1 = "amazon.titan-text-express-v1"
    AMAZON_TITAN_EMBED_TEXT_V1 = "amazon.titan-embed-text-v1"
    AMAZON_TITAN_EMBED_IMAGE_V1 = "amazon.titan-embed-image-v1"
    AMAZON_TITAN_IMAGE_GENERATOR_V1 = "amazon.titan-image-generator-v1"
    STABILITY_STABLE_DIFFUSION_XL = "stability.stable-diffusion-xl"
    STABILITY_STABLE_DIFFUSION_XL_V0 = "stability.stable-diffusion-xl-v0"
    STABILITY_STABLE_DIFFUSION_XL_V1 = "stability.stable-diffusion-xl-v1"
    AI21_J2_MID = "ai21.j2-mid"
    AI21_J2_MID_V1 = "ai21.j2-mid-v1"
    AI21_J2_ULTRA = "ai21.j2-ultra"
    AI21_J2_ULTRA_V1 = "ai21.j2-ultra-v1"
    ANTHROPIC_CLAUDE_INSTANT_V1 = "anthropic.claude-instant-v1"
    ANTHROPIC_CLAUDE_V1 = "anthropic.claude-v1"
    ANTHROPIC_CLAUDE_V2 = "anthropic.claude-v2"
    ANTHROPIC_CLAUDE_V2_1 = "anthropic.claude-v2:1"
    ANTHROPIC_CLAUDE_3_HAIKU_20240307_V1_0 = "anthropic.claude-3-haiku-20240307-v1:0"
    ANTHROPIC_CLAUDE_3_SONNET_20240229_V1_0 = "anthropic.claude-3-sonnet-20240229-v1:0"
    MISTRAL_MISTRAL_7B_INSTRUCT_V0_2 = "mistral.mistral-7b-instruct-v0:2"
    MISTRAL_MIXTRAL_8X7B_INSTRUCT_V0_1 = "mistral.mixtral-8x7b-instruct-v0:1"
    COHERE_COMMAND_TEXT_V14 = "cohere.command-text-v14"
    COHERE_COMMAND_LIGHT_TEXT_V14 = "cohere.command-light-text-v14"
    COHERE_EMBED_ENGLISH_V3 = "cohere.embed-english-v3"
    COHERE_EMBED_MULTILINGUAL_V3 = "cohere.embed-multilingual-v3"
    META_LLAMA2_13B_CHAT_V1 = "meta.llama2-13b-chat-v1"
    META_LLAMA2_70B_CHAT_V1 = "meta.llama2-70b-chat-v1"


class EmbeddingType(Enum):
    DOCUMENT = 0
    QUERY = 1


@define(kw_only=True)
class StreamDetails:
    """Details for the stream responses"""

    stream: Iterable = field(default=None)
    """An iterable providing the tokens as they get generated"""
    prompt: str = field(default="")
    """The prompt that is being sent to the model"""
    body: str = field(default="")
    """The prompt that is being sent to the model"""
    latency: float = field(default=0.0)
    """The latency for the invocation"""


@define(kw_only=True)
class CompletionDetails:
    output: str = field(default="")
    """The text being generated by the model"""
    response: Dict[str, any] = field(factory=dict)
    """The response as a dictionary"""
    prompt: str = field(default="")
    """The prompt that is being sent to the model"""
    body: str = field(default="")
    """The prompt that is being sent to the model"""
    latency: float = field(default=0.0)
    """The latency for the invocation"""


@define()
class MessageRole:
    HUMAN = "human"
    ASSISTANT = "assistant"
    SYSTEM = "system"


@define()
class Message:
    role: MessageRole = field(default=MessageRole.HUMAN)
    content: str = field(default="")


@define()
class Human(Message):
    role: MessageRole = field(default=MessageRole.HUMAN)
    images: list[Image] = field(default=[])


@define()
class Assistant(Message):
    role: MessageRole = field(default=MessageRole.ASSISTANT)


@define()
class System(Message):
    role: MessageRole = field(default=MessageRole.SYSTEM)


@define(kw_only=True)
class BedrockFoundationModel:
    """Abstract class for all foundation models exposed via Bedrock.

    To add a new FM, inherit from this class and implement the abstract methods:

    `get_body`, `get_text`, `process_response`, `validate_extra_args`, `model_id`
    """

    top_p: float = field(default=1)
    """**Top P** - Top P defines a cut off based on the sum of probabilities of the potential choices. 
    If you set Top P below 1.0, the model considers the most probable options and ignores less probable ones. 
    Top P is similar to Top K, but instead of capping the number of choices, it caps choices based on the sum 
    of their probabilities.

    For the example prompt "I hear the hoof beats of ," you might want the model to provide "horses," "zebras," 
    or "unicorns" as the next word. If you set the temperature to its maximum, without capping Top K or Top P, 
    you increase the probability of getting unusual results such as "unicorns." If you set the temperature to 0, 
    you increase the probability of "horses." If you set a high temperature and reduce the value of Top K or Top P, 
    you increase the probability of "horses" or "zebras," and decrease the probability of "unicorns".
    """

    temperature: float = field(default=0.7)
    """**Temperature** - Large language models use probability to construct the words in a sequence. 
    For any given sequence, there is a probability distribution of options for the next word in the sequence. 
    When you set the temperature closer to zero, the model tends to select the higher-probability words. 
    When you set the temperature further away from zero, the model may select a lower-probability word.

    In technical terms, the temperature modulates the probability density function for the next tokens, 
    implementing the temperature sampling technique. This parameter can deepen or flatten the density function curve. 
    A lower value results in a steeper curve with more deterministic responses, and a higher value results in a flatter 
    curve with more random responses."""

    max_token_count: int = field(default=500)
    """Configures the maximum number of tokens generated by an invocation"""

    stop_sequences: List[str] = field(factory=list)
    """**Stop sequence** - A stop sequence is a sequence of characters. If the model encounters a stop sequence, 
    it stops generating further tokens. Different models support different types of characters in a stop sequence, 
    different maximum sequence lengths, and may support the definition of multiple stop sequences."""

    extra_args: Dict[str, Any] = field(factory=dict)
    """A dictionary containing the extra arguments to pass to the model that are not supported via the common
    paramters."""

    session: boto3.Session = field(
        default=Factory(lambda: boto3.Session()), kw_only=True
    )
    """A `boto3.Session` object to use to create an instance of the Bedrock client"""

    _client: Any = field(
        default=Factory(
            lambda self: self.session.client("bedrock-runtime"),
            takes_self=True,
        ),
        kw_only=True,
    )
    """Instance of the Bedrock data plane client to use. By default it created one using the session"""

    _client_ops: Any = field(
        default=Factory(
            lambda self: self.session.client("bedrock"),
            takes_self=True,
        ),
        kw_only=True,
    )
    """Instance of the Bedrock control plane client to use. By default it created one using the session"""

    _model_id: str = field(default=None)

    @classmethod
    def _validate_model_id(cls, model_id: str) -> bool:
        return model_id.startswith(cls.family())

    @classmethod
    def from_id(cls, model_id: str | Model, **kwargs):
        """Instantiates a new model of the given class

        Args:
            model_id (str | Model): The modelId as a string or as a Model enum value

        Raises:
            BedrockArgsError: Raises an exception if the model is not compatible

        Returns:
            The FM specific model
        """

        if type(model_id) is Model:
            model_id = model_id.value
        if not cls._validate_model_id(model_id):
            raise BedrockArgsError(
                f"model_id {model_id} not compatible with by {cls.family}"
            )
        model = cls(**kwargs)
        model._model_id = model_id
        return model

    def list_model_ids(self) -> List[str]:
        models = self._client_ops.list_foundation_models()["modelSummaries"]
        return [m["modelId"] for m in models if m["modelId"].startswith(self.family())]

    @abstractmethod
    def family(cls) -> str:
        """Override this method in subclasses to return the model family.

        Returns:
            str: the model family
        """
        ...

    @abstractmethod
    def validate_extra_args(self, extra_args: Dict[str, Any]):
        """Override this method in the subclasses to provide a custom validator for the
        extra arguments to be passed to the model.

        Args:
            extra_args (Dict[str, Any]): A dictionary containing the extra arguments
        """
        ...

    @overload
    def generate(
        self,
        prompt: str,
        *,
        top_p: float = None,
        temperature: float = None,
        max_token_count: int = None,
        stop_sequences: List[str] = [],
        extra_args: Dict[str, Any] = None,
        details: Literal[True],
        stream: Literal[True],
    ) -> StreamDetails: ...

    @overload
    def generate(
        self,
        prompt: str,
        *,
        top_p: float = None,
        temperature: float = None,
        max_token_count: int = None,
        stop_sequences: List[str] = [],
        extra_args: Dict[str, Any] = None,
        details: Literal[True],
        stream: Literal[False],
    ) -> CompletionDetails: ...

    @overload
    def generate(
        self,
        prompt: str,
        *,
        top_p: float = None,
        temperature: float = None,
        max_token_count: int = None,
        stop_sequences: List[str] = [],
        extra_args: Dict[str, Any] = None,
        details: Literal[False],
        stream: Literal[False],
    ) -> List[str]: ...

    @overload
    def generate(
        self,
        prompt: str,
        *,
        top_p: float = None,
        temperature: float = None,
        max_token_count: int = None,
        stop_sequences: List[str] = [],
        extra_args: Dict[str, Any] = None,
        details: Literal[False],
        stream: Literal[True],
    ) -> Iterable: ...

    def generate(
        self,
        prompt: str,
        *,
        top_p: float = None,
        temperature: float = None,
        max_token_count: int = None,
        stop_sequences: List[str] = [],
        extra_args: Dict[str, Any] = None,
        details: bool = False,
        stream: bool = False,
    ) -> StreamDetails | CompletionDetails | List[str] | Iterable:
        """Generates a response from the model based on the prompt and the additional optional arguments

        Args:
            prompt (str): the user prompt
            top_p (float, optional): The Top P value. Defaults to the value set on the model instance.
            temperature (float, optional): The temperature for the generation. Defaults to the value set on the model instance.
            max_token_count (int, optional): Max number of tokens to return. Defaults to the value set on the model instance.
            stop_sequences (List[str], optional): The list of stop words. Defaults to the value set on the model instance.
            extra_args (Dict[str, Any], optional): Model specific extra arguments. Defaults to the value set on the model instance.

        Returns:
            Dict[str, Any]: A dictionary containing the output from the model as Dictionary, the prompt, the body passed to the model an the inference time.
        """
        if extra_args is None:
            extra_args = dict(self.extra_args)
        self.validate_extra_args(extra_args)
        logger.debug(f"extra_args = {extra_args}")
        if stop_sequences is None:
            stop_sequences = list(self.stop_sequences)

        logger.debug(f"stop_word = {stop_sequences}")
        body = self.get_body(
            prompt,
            top_p or self.top_p,
            temperature or self.temperature,
            max_token_count or self.max_token_count,
            stop_sequences,
            extra_args,
            stream,
        )
        logger.debug(f"Body= {body}")
        t = time.time()
        try:
            if stream:
                resp = self._client.invoke_model_with_response_stream(
                    modelId=self._model_id,
                    body=body,
                    contentType=CONTENT_TYPE_APPLICATION_JSON,
                    accept="*/*",
                )
            else:
                resp = self._client.invoke_model(
                    modelId=self._model_id,
                    body=body,
                    contentType=CONTENT_TYPE_APPLICATION_JSON,
                    accept="*/*",
                )
        except Exception as ex:
            raise ex

        if details:
            if stream:
                return StreamDetails(
                    stream=self._get_text_stream(resp["body"]),
                    prompt=prompt,
                    body=body,
                    latency=time.time() - t,
                )

            else:
                out_body = json.loads(resp["body"].read())
                return CompletionDetails(
                    output=self.process_response_body(out_body),
                    response=out_body,
                    prompt=prompt,
                    body=body,
                    latency=time.time() - t,
                )
        else:
            if stream:
                return self._get_text_stream(resp["body"])
            else:
                out_body = json.loads(resp["body"].read())
                return self.process_response_body(out_body)

    def chat(
        self,
        conversation: List[Human | Assistant | System],
        *,
        top_p: float = None,
        temperature: float = None,
        max_token_count: int = None,
        stop_sequences: List[str] = [],
        extra_args: Dict[str, Any] = None,
        details: bool = False,
        stream: bool = False,
    ) -> StreamDetails | CompletionDetails | List[str] | Iterable:
        if len(conversation) == 0:
            return [""]
        s = 0
        if conversation[0].role == MessageRole.SYSTEM:
            s = 1
        if not all([m.role == MessageRole.HUMAN for m in conversation[s + 0 :: 2]]):
            raise ValueError(
                "Human messages are not alternating correctly in the conversation"
            )
        if not all([m.role == MessageRole.ASSISTANT for m in conversation[s + 1 :: 2]]):
            raise ValueError(
                "Assistant messages are not alternating correctly in the conversation"
            )
        if conversation[-1].role != MessageRole.HUMAN:
            raise ValueError("Last messages in the conversation should be Human")

        prompt = self.get_chat_prompt(conversation=conversation)

        return self.generate(
            prompt=prompt,
            top_p=top_p,
            temperature=temperature,
            max_token_count=max_token_count,
            stop_sequences=stop_sequences,
            extra_args=extra_args,
            details=details,
            stream=stream,
        )

    def get_chat_prompt(
        self, conversation: List[Human | Assistant | System]
    ) -> str | list:
        raise BedrockArgsError("This model does not support chat mode")

    @abstractmethod
    def get_body(
        self,
        prompt: str,
        top_p: float,
        temperature: float,
        max_token_count: int,
        stop_sequences: List[str],
        extra_args: Dict[str, Any],
        stream: bool,
    ) -> str | dict:
        """Override this method in the model class to generate the stringified body to be passed to
        [InvokeModel](https://docs.aws.amazon.com/bedrock/latest/APIReference/API_runtime_InvokeModel.html).

        Args:
            prompt (str): the user prompt
            top_p (float): the top P value
            temperature (float): the temperature value
            max_token_count (int): the max token count
            stop_sequences (List[str]): the list of stop words
            extra_args (Dict[str, Any]): a dictionary containing the extra arguments to pass to the model
            stream (bool): a boolean indicating if a streaming response is required

        Returns:
            str: the stringified body to be passed to `InvokeModel`
        """
        ...

    @abstractmethod
    def process_response_body(self, body: Dict[str, Any]) -> List[str]:
        """Override this method in the model class to processes the response from the `InvokeModel` and extract the generated text.

        Args:
            resp (Dict[str, Any]): the response returned by InvokeModel as a dictionary.

        Returns:
            List[str]: The list of text generations
        """
        ...

    def _get_text_stream(self, stream: EventStream) -> Iterator[str]:
        """Internal method to return the stream of tokens

        Args:
            stream (EventStream): _description_

        Returns:
            Iterable[str]: _description_

        Yields:
            Iterator[Iterable[str]]: _description_
        """

        for e in stream:
            yield self.get_text(json.loads(e["chunk"]["bytes"]))

    @abstractmethod
    def get_text(self, body: Dict[str, Any]) -> str:
        """Override this method in the concrete class to extract the text from the answer

        Args:
            body (Dict[str, Any]): the body as a dictionary

        Returns:
            str: the generated text
        """
        ...


@define(kw_only=True)
class BedrockEmbeddingsModel:
    verbose: bool = field(default=False)
    session: boto3.Session = field(
        default=Factory(lambda: boto3.Session()),
    )
    _client: Any = field(
        default=Factory(
            lambda self: self.session.client("bedrock-runtime"),
            takes_self=True,
        ),
    )
    _client_ops: Any = field(
        default=Factory(
            lambda self: self.session.client("bedrock"),
            takes_self=True,
        ),
    )

    @classmethod
    def _validate_model_id(cls, model_id: str) -> bool:
        return model_id.startswith(cls.family())

    @classmethod
    def from_id(cls, model_id: str | Model, **kw):
        if type(model_id) is Model:
            model_id = model_id.value

        if not cls._validate_model_id(model_id):
            raise BedrockArgsError(
                f"model_id {model_id} not compatible with by {cls.family}"
            )
        model = cls(**kw)
        model._model_id = model_id
        return model

    def list_model_ids(self) -> List[str]:
        models = self._client_ops.list_foundation_models()["modelSummaries"]
        return [m["modelId"] for m in models if m["modelId"].startswith(self.family())]

    @abstractmethod
    def family(cls) -> str:
        """Override this method in subclasses to return the model family.

        Returns:
            str: the model family
        """
        ...

    def generate_for_documents(self, data: List[str]) -> List[List[float]]:
        """Generate embedding for 1 or multiple documents.
        Providing multiple documents in a single call must be supported by the model used

        Args:
            data (List[str]): The document passages to get the embedding for

        Returns:
            List[List[float]]: A list of embeddings vectors, one for each passage
        """
        return self.generate(data, type=EmbeddingType.DOCUMENT)

    def generate_for_query(self, data: str) -> List[float]:
        """Generate the embedding for the query.

        Args:
            data (str): The query for which one wants the embedding

        Returns:
            List[float]: The embedding vector
        """
        return self.generate([data], type=EmbeddingType.QUERY)[0]

    def generate(
        self, data: List[str], *, type: EmbeddingType = EmbeddingType.DOCUMENT
    ) -> List[List[float]]:
        """Generate the embedding vectors for a list of passages or a list of queries.

        Args:
            data (List[str]): the list of passages or queries
            type (EmbeddingType, optional): The type of embedding to generate. Defaults to EmbeddingType.DOCUMENT.

        Returns:
            List[List[float]]: A list of embedding vectors
        """

        body = self.get_body(data, type)

        logger.debug("Body:")
        logger.debug(body)
        response = self._client.invoke_model(
            modelId=self._model_id,
            body=body,
            accept="*/*",
            contentType="application/json",
        )
        return self.parse_response(response)

    @abstractmethod
    def get_body(self, data: List[str], type: EmbeddingType) -> str: ...

    @abstractmethod
    def parse_response(self, response: bytes) -> List[List[float]]: ...
